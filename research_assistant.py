# -*- coding: utf-8 -*-
"""Research_Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZnMsD1S0sroX827WOayLXnqYVcOkRCeY
"""

!pip install sentence-transformers chromadb gradio transformers accelerate arxiv

import gradio as gr
import arxiv
import torch
from sentence_transformers import SentenceTransformer
import chromadb
from transformers import pipeline

!pip install arxiv==2.1.0

chroma_client = chromadb.Client()
collection = chroma_client.create_collection(name="research_papers")
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# using Hugging Face LLM
hf_model = pipeline(
    "text-generation",
    model="HuggingFaceH4/zephyr-7b-beta",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

print("Models and vector store initialized!")

def fetch_papers_from_arxiv(topic, max_results=5):
    """
    Automatically fetch recent papers from arXiv based on the topic.
    """
    search = arxiv.Search(
        query=topic,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.Relevance
    )

    papers = []
    for result in search.results():
        papers.append({
            "title": result.title,
            "summary": result.summary[:500]
        })
    return papers

# Example Test
papers = fetch_papers_from_arxiv("transformer architecture")
for p in papers[:2]:
    print(f"üßæ {p['title']}\n‚û°Ô∏è {p['summary'][:150]}...\n")

def store_papers(papers):
    for i, p in enumerate(papers):
        collection.add(
            ids=[str(len(collection.get()['ids']) + i)],
            documents=[p["summary"]],
            metadatas=[{"title": p["title"]}]
        )
    print(f"‚úÖ Stored {len(papers)} papers in ChromaDB.")

# Example: Fetch and store
store_papers(fetch_papers_from_arxiv("artificial intelligence"))

def research_chat(message, history):
    try:
        context = " ".join([m[0] + " " + m[1] for m in history[-3:]]) + " " + message

        #Query top relevant papers
        results = collection.query(query_texts=[context], n_results=3)
        top_docs = results['documents'][0]
        top_titles = [m['title'] for m in results['metadatas'][0]]

        # Build base summary
        base_response = f"üìö Based on your query, I found {len(top_docs)} related papers:\n"
        for i, (t, d) in enumerate(zip(top_titles, top_docs)):
            base_response += f"\n{i+1}. **{t}** ‚Äî {d[:150]}...\n"

        # Generate human-like answer
        prompt = (
            "You are a research assistant summarizing papers simply and helpfully.\n"
            f"User context: {context}\n\n"
            f"Relevant research:\n{base_response}\n\nAnswer clearly:"
        )

        output = hf_model(prompt, max_new_tokens=200, do_sample=True)[0]['generated_text']
        answer = output.split("Answer clearly:")[-1].strip()
        return answer

    except Exception as e:
        return f"‚ö†Ô∏è Error: {str(e)}"

with gr.Blocks(theme="default") as demo:
    gr.Markdown("## üß† Smart AI Research Assistant")

    topic_input = gr.Textbox(label="Enter Research Topic")
    fetch_btn = gr.Button("üì° Fetch & Store Papers")
    fetch_output = gr.Textbox(label="Fetched Paper Titles")

    chatbot = gr.Chatbot(label="Chat about your research topic")
    msg = gr.Textbox(label="Ask your research question...")
    clear = gr.ClearButton([msg, chatbot])

    state = gr.State([])

    def fetch_and_store(topic):
        papers = fetch_papers_from_arxiv(topic)
        store_papers(papers)
        titles = "\n".join([p["title"] for p in papers])
        return f" Added papers:\n{titles}"

    fetch_btn.click(fetch_and_store, inputs=topic_input, outputs=fetch_output)
    msg.submit(research_chat, inputs=[msg, state], outputs=chatbot, preprocess=False)

demo.launch(share=True)

